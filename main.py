import os
import sys
import json
import asyncio
import aiohttp
import tiktoken
from typing import List, Optional
import argparse
from pathlib import Path
import time
from datetime import datetime

class GeminiFileProcessor:
    def __init__(self, api_key: str, model: str = "gemini-2.0-flash", paid_tier: bool = False):
        self.api_key = api_key
        self.model = model
        self.paid_tier = paid_tier
        self.encoding = tiktoken.get_encoding("cl100k_base")
        
        # –õ–∏–º–∏—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —É—Ä–æ–≤–Ω–µ–π –∏ –º–æ–¥–µ–ª–µ–π
        if paid_tier:
            self.rate_limits = {
                "gemini-2.0-flash": {"rpm": 2000, "tpm": 4000000, "rpd": 10000000},
                "gemini-2.0-flash-lite": {"rpm": 4000, "tpm": 4000000, "rpd": 10000000},
                "gemini-2.5-flash": {"rpm": 1000, "tpm": 1000000, "rpd": 10000},
                "gemini-2.5-flash-lite": {"rpm": 4000, "tpm": 4000000, "rpd": 10000000},
                "gemini-2.5-pro": {"rpm": 150, "tpm": 2000000, "rpd": 10000},
                "gemini-1.5-flash": {"rpm": 2000, "tpm": 4000000, "rpd": None},
                "gemini-1.5-pro": {"rpm": 1000, "tpm": 4000000, "rpd": None}
            }
        else:
            # Free Tier –ª–∏–º–∏—Ç—ã
            self.rate_limits = {
                "gemini-2.0-flash": {"rpm": 15, "tpm": 1000000, "rpd": 200},
                "gemini-2.0-flash-lite": {"rpm": 30, "tpm": 1000000, "rpd": 200},
                "gemini-2.5-flash": {"rpm": 10, "tpm": 250000, "rpd": 250},
                "gemini-2.5-flash-lite": {"rpm": 15, "tpm": 250000, "rpd": 1000},
                "gemini-2.5-pro": {"rpm": 5, "tpm": 250000, "rpd": 100},
                "gemini-1.5-flash": {"rpm": 15, "tpm": 250000, "rpd": 50},
                "gemini-1.5-pro": {"rpm": 2, "tpm": 32000, "rpd": 50}
            }
        
        self.current_limits = self.rate_limits.get(model, {"rpm": 15, "tpm": 250000, "rpd": 200})
        
        tier_name = "–ü–ª–∞—Ç–Ω—ã–π" if paid_tier else "Free"
        print(f"ü§ñ –ú–æ–¥–µ–ª—å: {model}")
        print(f"üí≥ –£—Ä–æ–≤–µ–Ω—å: {tier_name}")
        print(f"üìä –õ–∏–º–∏—Ç—ã: {self.current_limits['rpm']} RPM, {self.current_limits['tpm']:,} TPM")
        
    def count_tokens(self, text: str) -> int:
        """–ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        return len(self.encoding.encode(text))
    
    def split_into_chunks(self, text: str, prompt: str, max_chunk_tokens: int) -> List[str]:
        """–†–∞–∑–±–∏–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ —Å —É—á–µ—Ç–æ–º –ª–∏–º–∏—Ç–æ–≤ –∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ 10,000 —Ç–æ–∫–µ–Ω–æ–≤"""
        prompt_tokens = self.count_tokens(prompt)
        available_tokens = max_chunk_tokens - prompt_tokens - 2000  # —Ä–µ–∑–µ—Ä–≤ –¥–ª—è –æ—Ç–≤–µ—Ç–∞
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ 10,000 —Ç–æ–∫–µ–Ω–æ–≤
        min_chunk_tokens = 10000
        
        if available_tokens <= min_chunk_tokens:
            raise ValueError(f"–†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ —Å–ª–∏—à–∫–æ–º –º–∞–ª. –¢—Ä–µ–±—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º {min_chunk_tokens + prompt_tokens + 2000} —Ç–æ–∫–µ–Ω–æ–≤")
        
        chunks = []
        current_chunk = ""
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –∞–±–∑–∞—Ü–∞–º
        paragraphs = text.split('\n\n')
        
        for paragraph in paragraphs:
            paragraph_tokens = self.count_tokens(paragraph)
            current_chunk_tokens = self.count_tokens(current_chunk)
            
            # –ï—Å–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —ç—Ç–æ–≥–æ –∞–±–∑–∞—Ü–∞ –ø—Ä–µ–≤—ã—Å–∏—Ç –ª–∏–º–∏—Ç
            if current_chunk_tokens + paragraph_tokens > available_tokens:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç–∏–≥–ª–∏ –ª–∏ –º—ã –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
                if current_chunk_tokens >= min_chunk_tokens:
                    chunks.append(current_chunk.strip())
                    current_chunk = paragraph
                elif current_chunk:
                    # –ï—Å–ª–∏ —á–∞–Ω–∫ –º–µ–Ω—å—à–µ –º–∏–Ω–∏–º—É–º–∞, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –¥–æ–±–∞–≤–ª—è—Ç—å
                    # –†–∞–∑–±–∏–≤–∞–µ–º –±–æ–ª—å—à–æ–π –∞–±–∑–∞—Ü –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
                    sentences = paragraph.split('. ')
                    for sentence in sentences:
                        sentence_with_dot = sentence + ". " if not sentence.endswith('.') else sentence + " "
                        
                        if self.count_tokens(current_chunk + sentence_with_dot) > available_tokens:
                            if self.count_tokens(current_chunk) >= min_chunk_tokens:
                                chunks.append(current_chunk.strip())
                                current_chunk = sentence_with_dot
                            else:
                                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ, –µ—Å–ª–∏ —á–∞–Ω–∫ –µ—â–µ –º–∞–ª
                                current_chunk += sentence_with_dot
                        else:
                            current_chunk += sentence_with_dot
                else:
                    # –ü–µ—Ä–≤—ã–π –∞–±–∑–∞—Ü —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π, —Ä–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
                    sentences = paragraph.split('. ')
                    for sentence in sentences:
                        sentence_with_dot = sentence + ". " if not sentence.endswith('.') else sentence + " "
                        
                        if self.count_tokens(current_chunk + sentence_with_dot) > available_tokens:
                            if current_chunk and self.count_tokens(current_chunk) >= min_chunk_tokens:
                                chunks.append(current_chunk.strip())
                                current_chunk = sentence_with_dot
                            else:
                                # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Å–ª–æ–≤–∞–º, –µ—Å–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–µ
                                words = sentence.split()
                                for word in words:
                                    word_with_space = word + " "
                                    if self.count_tokens(current_chunk + word_with_space) > available_tokens:
                                        if current_chunk and self.count_tokens(current_chunk) >= min_chunk_tokens:
                                            chunks.append(current_chunk.strip())
                                            current_chunk = word_with_space
                                        else:
                                            current_chunk += word_with_space
                                    else:
                                        current_chunk += word_with_space
                        else:
                            current_chunk += sentence_with_dot
            else:
                current_chunk += paragraph + "\n\n"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk:
            current_chunk_tokens = self.count_tokens(current_chunk)
            if current_chunk_tokens >= min_chunk_tokens or not chunks:
                chunks.append(current_chunk.strip())
            elif chunks:
                # –ï—Å–ª–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫ —Å–ª–∏—à–∫–æ–º –º–∞–ª, –¥–æ–±–∞–≤–ª—è–µ–º –∫ –ø—Ä–µ–¥—ã–¥—É—â–µ–º—É
                chunks[-1] += "\n\n" + current_chunk.strip()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ —á–∞–Ω–∫–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É —Ä–∞–∑–º–µ—Ä—É
        final_chunks = []
        for i, chunk in enumerate(chunks):
            chunk_tokens = self.count_tokens(chunk)
            if chunk_tokens < min_chunk_tokens and i < len(chunks) - 1:
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å —Å–ª–µ–¥—É—é—â–∏–º —á–∞–Ω–∫–æ–º
                chunks[i + 1] = chunk + "\n\n" + chunks[i + 1]
            else:
                final_chunks.append(chunk)
                
        print(f"üìè –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞: {min_chunk_tokens:,} —Ç–æ–∫–µ–Ω–æ–≤")
        for i, chunk in enumerate(final_chunks):
            chunk_tokens = self.count_tokens(chunk)
            print(f"   –ß–∞–Ω–∫ {i + 1}: {chunk_tokens:,} —Ç–æ–∫–µ–Ω–æ–≤")
            
        return final_chunks
    
    async def process_chunk_with_retry(self, session: aiohttp.ClientSession, chunk: str, prompt: str, chunk_index: int, max_retries: int = 3) -> dict:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–∞ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ –ø—Ä–∏ rate limit"""
        for attempt in range(max_retries):
            try:
                result = await self.process_chunk(session, chunk, prompt, chunk_index)
                if result["success"]:
                    return result
                    
                # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ 429 (rate limit), –∂–¥–µ–º –∏ –ø–æ–≤—Ç–æ—Ä—è–µ–º
                if "429" in str(result.get("error", "")):
                    if self.paid_tier:
                        wait_time = 15 * (attempt + 1)  # –ú–µ–Ω—å—à–µ –æ–∂–∏–¥–∞–Ω–∏—è –¥–ª—è –ø–ª–∞—Ç–Ω–æ–≥–æ
                    else:
                        wait_time = 60 * (attempt + 1)  # –ë–æ–ª—å—à–µ –¥–ª—è free tier
                    
                    print(f"‚è≥ Rate limit –¥–ª—è —á–∞–Ω–∫–∞ {chunk_index + 1}, –æ–∂–∏–¥–∞–Ω–∏–µ {wait_time}—Å (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}/{max_retries})")
                    await asyncio.sleep(wait_time)
                    continue
                else:
                    return result
                    
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –ø–æ–ø—ã—Ç–∫–µ {attempt + 1} –¥–ª—è —á–∞–Ω–∫–∞ {chunk_index + 1}: {e}")
                if attempt < max_retries - 1:
                    await asyncio.sleep(5 * (attempt + 1))
                    
        return {
            "chunk_index": chunk_index,
            "success": False,
            "error": f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –ø–æ—Å–ª–µ {max_retries} –ø–æ–ø—ã—Ç–æ–∫"
        }
    
    async def process_chunk(self, session: aiohttp.ClientSession, chunk: str, prompt: str, chunk_index: int) -> dict:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ —á–∞–Ω–∫–∞ —á–µ—Ä–µ–∑ Gemini API"""
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{self.model}:generateContent?key={self.api_key}"
        
        headers = {"Content-Type": "application/json"}
        
        data = {
            "contents": [{"parts": [{"text": f"{prompt}\n\n–¢–µ–∫—Å—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏:\n{chunk}"}]}],
            "generationConfig": {
                "temperature": 0.7,
                "topK": 40,
                "topP": 0.95,
                "maxOutputTokens": 8192 if self.paid_tier else 4096,
                "candidateCount": 1
            },
            "safetySettings": [
                {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
                {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"}
            ]
        }
        
        try:
            timeout = aiohttp.ClientTimeout(total=300 if self.paid_tier else 180)
            async with session.post(url, headers=headers, json=data, timeout=timeout) as response:
                if response.status == 200:
                    result = await response.json()
                    
                    if "candidates" in result and len(result["candidates"]) > 0:
                        candidate = result["candidates"][0]
                        if "content" in candidate and "parts" in candidate["content"]:
                            content = candidate["content"]["parts"][0]["text"]
                            
                            input_tokens = self.count_tokens(chunk + prompt)
                            output_tokens = self.count_tokens(content)
                            total_tokens = input_tokens + output_tokens
                            
                            print(f"‚úÖ –ß–∞–Ω–∫ {chunk_index + 1}: {input_tokens:,} + {output_tokens:,} = {total_tokens:,} —Ç–æ–∫–µ–Ω–æ–≤")
                            return {
                                "chunk_index": chunk_index,
                                "success": True,
                                "content": content,
                                "tokens_used": total_tokens,
                                "input_tokens": input_tokens,
                                "output_tokens": output_tokens
                            }
                        else:
                            return {"chunk_index": chunk_index, "success": False, "error": "–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç Gemini"}
                    else:
                        return {"chunk_index": chunk_index, "success": False, "error": "–ù–µ—Ç –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ"}
                else:
                    error_text = await response.text()
                    return {"chunk_index": chunk_index, "success": False, "error": f"HTTP {response.status}: {error_text}"}
                    
        except Exception as e:
            return {"chunk_index": chunk_index, "success": False, "error": str(e)}
    
    async def process_file(self, file_path: str, prompt: str, output_path: str, chunk_size: int = 1000000, delay: int = 0, concurrent: int = 1):
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞ —Å –ø–æ–ª–Ω–æ–π –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏–µ–π"""
        print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞: {file_path}")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
            return False
        
        total_tokens = self.count_tokens(content)
        file_size_mb = len(content) / 1024 / 1024
        
        print(f"üìè –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {len(content):,} —Å–∏–º–≤–æ–ª–æ–≤ ({file_size_mb:.1f}MB)")
        print(f"üî¢ –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤: {total_tokens:,}")
        
        chunks = self.split_into_chunks(content, prompt, chunk_size)
        print(f"üì¶ –§–∞–π–ª —Ä–∞–∑–±–∏—Ç –Ω–∞ {len(chunks)} —á–∞–Ω–∫–æ–≤ (–º–∏–Ω. 10,000 —Ç–æ–∫–µ–Ω–æ–≤ –∫–∞–∂–¥—ã–π)")
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        estimated_time_per_chunk = 15 if not self.paid_tier else 8
        estimated_time = (len(chunks) * estimated_time_per_chunk) / concurrent + (len(chunks) * delay / concurrent)
        estimated_minutes = estimated_time / 60
        
        print(f"‚è±Ô∏è –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {estimated_minutes:.1f} –º–∏–Ω—É—Ç")
        print(f"üöÄ –ü–æ–ª–Ω–∞—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: {concurrent} –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
        
        # –°–æ–∑–¥–∞–µ–º —Å–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        semaphore = asyncio.Semaphore(concurrent)
        
        connector = aiohttp.TCPConnector(
            limit=concurrent * 3,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
            limit_per_host=concurrent * 2,
            ttl_dns_cache=300,
            use_dns_cache=True,
        )
        timeout = aiohttp.ClientTimeout(total=600)
        
        async def process_with_semaphore(session, chunk, i):
            async with semaphore:
                start_time = asyncio.get_event_loop().time()
                print(f"üîÑ –ù–∞—á–∏–Ω–∞–µ–º —á–∞–Ω–∫ {i + 1}/{len(chunks)} ({datetime.now().strftime('%H:%M:%S')})")
                
                result = await self.process_chunk_with_retry(session, chunk, prompt, i)
                
                end_time = asyncio.get_event_loop().time()
                duration = end_time - start_time
                
                if result.get("success"):
                    print(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω —á–∞–Ω–∫ {i + 1}/{len(chunks)} –∑–∞ {duration:.1f}—Å")
                else:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —á–∞–Ω–∫–µ {i + 1}/{len(chunks)}: {result.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}")
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –∑–∞–¥–µ—Ä–∂–∫—É —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∞ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞
                if delay > 0:
                    print(f"‚è∏Ô∏è –ó–∞–¥–µ—Ä–∂–∫–∞ {delay}—Å –ø–æ—Å–ª–µ —á–∞–Ω–∫–∞ {i + 1}...")
                    await asyncio.sleep(delay)
                
                return result
        
        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            # –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É, –¥–∞–∂–µ –ø—Ä–∏ concurrent=1
            print(f"üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º {len(chunks)} –∑–∞–¥–∞—á –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º {concurrent} –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö")
            
            start_time = time.time()
            
            # –°–æ–∑–¥–∞–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ —Å—Ä–∞–∑—É
            tasks = [process_with_semaphore(session, chunk, i) for i, chunk in enumerate(chunks)]
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            end_time = time.time()
            total_duration = end_time - start_time
            
            print(f"\n‚è±Ô∏è –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á: {total_duration / 60:.1f} –º–∏–Ω—É—Ç")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    results[i] = {
                        "chunk_index": i,
                        "success": False,
                        "error": str(result)
                    }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        successful_results = [r for r in results if isinstance(r, dict) and r.get("success")]
        failed_results = [r for r in results if isinstance(r, dict) and not r.get("success")]
        
        if failed_results:
            print(f"\n‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å {len(failed_results)} —á–∞–Ω–∫–æ–≤:")
            for failed in failed_results[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 5 –æ—à–∏–±–æ–∫
                print(f"   –ß–∞–Ω–∫ {failed.get('chunk_index', '?') + 1}: {failed.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}")
            if len(failed_results) > 5:
                print(f"   ... –∏ –µ—â–µ {len(failed_results) - 5} –æ—à–∏–±–æ–∫")
        
        if successful_results:
            try:
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∏–Ω–¥–µ–∫—Å—É —á–∞–Ω–∫–∞
                successful_results.sort(key=lambda x: x['chunk_index'])
                
                final_content = "\n\n".join([
                    f"=== –†–ï–ó–£–õ–¨–¢–ê–¢ –ß–ê–ù–ö–ê {r['chunk_index'] + 1} ===\n{r['content']}"
                    for r in successful_results
                ])
                
                Path(output_path).parent.mkdir(parents=True, exist_ok=True)
                
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(final_content)
                
                total_tokens = sum(r.get("tokens_used", 0) for r in successful_results)
                total_input_tokens = sum(r.get("input_tokens", 0) for r in successful_results)
                total_output_tokens = sum(r.get("output_tokens", 0) for r in successful_results)
                
                print(f"\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
                print(f"   üìä –£—Å–ø–µ—à–Ω–æ: {len(successful_results)}/{len(chunks)} —á–∞–Ω–∫–æ–≤")
                print(f"   üî¢ –í—Å–µ–≥–æ —Ç–æ–∫–µ–Ω–æ–≤: {total_tokens:,}")
                print(f"   üì• –í—Ö–æ–¥—è—â–∏–µ: {total_input_tokens:,}")
                print(f"   üì§ –ò—Å—Ö–æ–¥—è—â–∏–µ: {total_output_tokens:,}")
                print(f"   üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç: {output_path}")
                print(f"   ‚ö° –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–∞—Ü–∏–∏: {(len(chunks) * estimated_time_per_chunk) / total_duration:.1f}x")
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                result_size = len(final_content) / 1024
                print(f"   üìè –†–∞–∑–º–µ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {result_size:.1f}KB")
                
                return len(failed_results) == 0  # True –µ—Å–ª–∏ –≤—Å–µ —á–∞–Ω–∫–∏ —É—Å–ø–µ—à–Ω—ã
                
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {e}")
                return False
        else:
            print("\n‚ùå –ù–∏ –æ–¥–∏–Ω —á–∞–Ω–∫ –Ω–µ –±—ã–ª —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω")
            return False

def main():
    parser = argparse.ArgumentParser(description="–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ —á–µ—Ä–µ–∑ Gemini API")
    parser.add_argument("--file", required=True, help="–ü—É—Ç—å –∫ –≤—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É")
    parser.add_argument("--prompt", required=True, help="–ü—Ä–æ–º–ø—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    parser.add_argument("--output", required=True, help="–ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É")
    parser.add_argument("--model", default="gemini-2.0-flash", help="–ú–æ–¥–µ–ª—å Gemini")
    parser.add_argument("--chunk-size", type=int, default=1000000, help="–†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Ç–æ–∫–µ–Ω–∞—Ö (–º–∏–Ω. 12000)")
    parser.add_argument("--delay", type=int, default=0, help="–ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö")
    parser.add_argument("--concurrent", type=int, default=1, help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤")
    parser.add_argument("--paid-tier", action="store_true", help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏–º–∏—Ç—ã –ø–ª–∞—Ç–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è")
    parser.add_argument("--api-key", help="API –∫–ª—é—á Gemini")
    
    args = parser.parse_args()
    
    api_key = args.api_key or os.getenv("GEMINI_API_KEY")
    if not api_key:
        print("‚ùå –ù–µ —É–∫–∞–∑–∞–Ω API –∫–ª—é—á Gemini")
        print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é GEMINI_API_KEY –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ --api-key")
        sys.exit(1)
    
    if not os.path.exists(args.file):
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {args.file}")
        sys.exit(1)
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    if args.concurrent < 1 or args.concurrent > 20:
        print("‚ùå –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ—Ç 1 –¥–æ 20")
        sys.exit(1)
    
    # –ù–æ–≤–æ–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Å —É—á–µ—Ç–æ–º –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ —á–∞–Ω–∫–∞
    if args.chunk_size < 12000 or args.chunk_size > 5000000:
        print("‚ùå –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –æ—Ç 12,000 –¥–æ 5,000,000 —Ç–æ–∫–µ–Ω–æ–≤")
        print("   (–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä 10,000 + —Ä–µ–∑–µ—Ä–≤ –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞ –∏ –æ—Ç–≤–µ—Ç–∞)")
        sys.exit(1)
    
    processor = GeminiFileProcessor(api_key, args.model, args.paid_tier)
    
    start_time = time.time()
    
    try:
        success = asyncio.run(processor.process_file(
            args.file, 
            args.prompt, 
            args.output,
            args.chunk_size,
            args.delay,
            args.concurrent
        ))
        
        end_time = time.time()
        duration = end_time - start_time
        duration_minutes = duration / 60
        
        print(f"\nüèÅ –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {duration_minutes:.1f} –º–∏–Ω—É—Ç")
        
        if success:
            print("üéâ –í—Å–µ —á–∞–Ω–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
            sys.exit(0)
        else:
            print("‚ö†Ô∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —Å –æ—à–∏–±–∫–∞–º–∏")
            sys.exit(2)  # –ß–∞—Å—Ç–∏—á–Ω—ã–π —É—Å–ø–µ—Ö
            
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(1)
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
